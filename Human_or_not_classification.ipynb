{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac12c3d-f369-4ac6-96f4-9005f722c5bd",
      "metadata": {
        "id": "bac12c3d-f369-4ac6-96f4-9005f722c5bd",
        "outputId": "d96d96c8-2b45-4612-820e-15e204b18774"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.16 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "def load_csv(csv_file, img_dir):\n",
        "    data = pd.read_csv(csv_file)\n",
        "    img_paths = [os.path.join(img_dir, img) for img in data['id']]\n",
        "    labels = data['target_people'].values\n",
        "    return img_paths, labels\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def prepare_dataloader(csv_file, img_dir, transforms, batch_size=128, shuffle=True):\n",
        "    img_paths, labels = load_csv(csv_file, img_dir)\n",
        "    dataset = CustomDataset(img_paths, labels, transform=transforms)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1087573f-162b-420b-a30f-cd8262bca648",
      "metadata": {
        "id": "1087573f-162b-420b-a30f-cd8262bca648"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision import models as torchvision_models\n",
        "\n",
        "def get_model(model_name, num_classes=2):\n",
        "    if model_name in ['efficientnet_b0', 'mobilenet_v2', 'mobilenet_v3']:\n",
        "        if model_name == 'efficientnet_b0':\n",
        "            model = torchvision_models.efficientnet_b0(weights='DEFAULT')\n",
        "            num_ftrs = model.classifier[1].in_features\n",
        "            model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "        elif model_name == 'mobilenet_v2':\n",
        "            model = torchvision_models.mobilenet_v2(weights='DEFAULT')\n",
        "            num_ftrs = model.classifier[1].in_features\n",
        "            model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "        elif model_name == 'mobilenet_v3':\n",
        "            model = torchvision_models.mobilenet_v3_large(weights='DEFAULT')\n",
        "            num_ftrs = model.classifier[3].in_features\n",
        "            model.classifier[3] = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    elif model_name in ['regnet', 'shufflenet_v2']:\n",
        "        if model_name == 'regnet':\n",
        "            model = timm.create_model('regnety_002', pretrained=True, num_classes=num_classes)\n",
        "        elif model_name == 'shufflenet_v2':\n",
        "            model = torchvision_models.shufflenet_v2_x1_0(weights='DEFAULT')\n",
        "            num_ftrs = model.fc.in_features\n",
        "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Model '{model_name}' is not supported.\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d1c663-8af7-40e7-8ee4-03c3b174156d",
      "metadata": {
        "id": "32d1c663-8af7-40e7-8ee4-03c3b174156d"
      },
      "outputs": [],
      "source": [
        "train_transforms = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=25),\n",
        "    A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0)),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "valid_transforms = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "train_csv = 'train.csv'\n",
        "valid_csv = 'valid.csv'\n",
        "train_dir = 'train'\n",
        "valid_dir = 'valid'\n",
        "\n",
        "train_loader = prepare_dataloader(train_csv, train_dir, train_transforms, batch_size=128, shuffle=True)\n",
        "valid_loader = prepare_dataloader(valid_csv, valid_dir, valid_transforms, batch_size=128, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9d3337-301e-4f7c-9ec3-d179b03a4a65",
      "metadata": {
        "id": "2d9d3337-301e-4f7c-9ec3-d179b03a4a65"
      },
      "outputs": [],
      "source": [
        "def load_model_weights(model_name, weights_path='efficientnet_b0'):\n",
        "    model = get_model(model_name)\n",
        "    model.load_state_dict(torch.load(weights_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d3aad5-3183-443c-a895-78cdf2c95ec3",
      "metadata": {
        "id": "97d3aad5-3183-443c-a895-78cdf2c95ec3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "def continue_training(train_loader, valid_loader, criterion, optimizer = None, model_name=None, model=None, scheduler=None, epochs=5, learn_r=0.001, weights_path='efficientnet_b0', new_save_path='updated_model_weights.pth'):\n",
        "    if not model:\n",
        "        model = load_model_weights(model_name, weights_path)\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model = model.to(device)\n",
        "\n",
        "    if not optimizer:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learn_r)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_losses, valid_losses = [], []\n",
        "    valid_accuracies, valid_roc_aucs = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
        "\n",
        "        for i, (images, labels) in loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            loop.set_postfix(loss=running_loss/(i+1))\n",
        "            if (running_loss/(i+1)) > 0.4 or loss > 0.4:\n",
        "                print(f\"Training stopped due to significant loss increase at Epoch {epoch+1}\")\n",
        "                return model\n",
        "                break\n",
        "\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_valid_labels, all_valid_preds, all_valid_probs = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valid_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "                all_valid_labels.extend(labels.cpu().numpy())\n",
        "                all_valid_preds.extend(preds.cpu().numpy())\n",
        "                all_valid_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "        valid_losses.append(val_loss / len(valid_loader))\n",
        "        valid_accuracy = accuracy_score(all_valid_labels, all_valid_preds)\n",
        "        valid_roc_auc = roc_auc_score(all_valid_labels, all_valid_probs)\n",
        "\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "        valid_roc_aucs.append(valid_roc_auc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss/len(valid_loader):.4f}, '\n",
        "              f'Validation Accuracy: {valid_accuracy:.4f}, ROC AUC: {valid_roc_auc:.4f}')\n",
        "        torch.save(model.state_dict(), f'resnet_50_weights_for_accuracy_{valid_accuracy}_valid_loss_{val_loss}')\n",
        "        print(f\"Updated model weights saved to resnet_50_weights_for_accuracy_{valid_accuracy}_valid_loss_{val_loss}\")\n",
        "\n",
        "        if valid_accuracy < 0.89:\n",
        "            print(f\"Training stopped due to significant loss increase at Epoch {epoch+1}\")\n",
        "            return model\n",
        "            break\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "    torch.save(model.state_dict(), new_save_path)\n",
        "    print(f\"Updated model weights saved to {new_save_path}\")\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5178e86-317b-4878-ba26-48914a8a4a5a",
      "metadata": {
        "id": "f5178e86-317b-4878-ba26-48914a8a4a5a"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "def get_subset_loader(loader, fraction=0.33):\n",
        "    dataset_size = len(loader.dataset)\n",
        "    subset_size = int(dataset_size * fraction)\n",
        "\n",
        "    indices = np.random.choice(dataset_size, subset_size, replace=False)  # Выбираем случайные индексы для подвыборки\n",
        "    subset = Subset(loader.dataset, indices)\n",
        "\n",
        "    subset_loader = DataLoader(subset, batch_size=loader.batch_size, shuffle=True)\n",
        "    return subset_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ccbe17-b29b-4db6-a2a8-190ab4aa1b8a",
      "metadata": {
        "id": "b0ccbe17-b29b-4db6-a2a8-190ab4aa1b8a"
      },
      "outputs": [],
      "source": [
        "def get_pretrained_resnet50(num_classes=2):\n",
        "    model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, num_classes)  # Выход на 2 класса\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=10, scheduler=None, early_stopping_threshold=1.3):\n",
        "    train_losses, valid_losses = [], []\n",
        "    valid_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valid_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        valid_loss = val_loss / len(valid_loader)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        valid_accuracies.append(accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "        torch.save(model.state_dict(), f'resnet_50_weights_for_accuracy_{accuracy}_valid_loss_{valid_loss}')\n",
        "        print(f\"Updated model weights saved to resnet_50_weights_for_accuracy_{accuracy}_valid_loss_{valid_loss}\")\n",
        "\n",
        "        if epoch > 0 and valid_loss > valid_losses[-2]:\n",
        "            print(f\"Training stopped due to significant loss increase at Epoch {epoch+1}\")\n",
        "            return model, train_losses, valid_losses, valid_accuracies\n",
        "            break\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "    return model, train_losses, valid_losses, valid_accuracies\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8119cf-abe2-4681-81bd-45faafa9c62b",
      "metadata": {
        "id": "5e8119cf-abe2-4681-81bd-45faafa9c62b"
      },
      "outputs": [],
      "source": [
        "def load_resnet_80_percent(path=None):\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, 2)  # Выход на 2 класса\n",
        "    if not path:\n",
        "        model.load_state_dict(torch.load('resnet_50_weights_best', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f95c54-cd60-431f-ac41-838741c77e9a",
      "metadata": {
        "id": "68f95c54-cd60-431f-ac41-838741c77e9a",
        "outputId": "a479a5e5-2384-49d1-8b4a-209535c8c4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "now BATCH is 16\n",
            "now BATCH is 16 and lr is 0.05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 5.3647, Validation Accuracy: 0.6009, ROC AUC: 0.5056\n",
            "Updated model weights saved to resnet_lr_0.05_batch16.pth\n",
            "now BATCH is 16 and lr is 0.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 1.0751, Validation Accuracy: 0.6029, ROC AUC: 0.4889\n",
            "Updated model weights saved to resnet_lr_0.01_batch16.pth\n",
            "now BATCH is 16 and lr is 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.6660, Validation Accuracy: 0.6029, ROC AUC: 0.5699\n",
            "Updated model weights saved to resnet_lr_0.005_batch16.pth\n",
            "now BATCH is 16 and lr is 0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.5521, Validation Accuracy: 0.7318, ROC AUC: 0.7890\n",
            "Updated model weights saved to resnet_lr_0.001_batch16.pth\n",
            "now BATCH is 16 and lr is 0.0005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.4605, Validation Accuracy: 0.8040, ROC AUC: 0.8594\n",
            "Updated model weights saved to resnet_lr_0.0005_batch16.pth\n",
            "now BATCH is 16 and lr is 0.0001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.3596, Validation Accuracy: 0.8516, ROC AUC: 0.9297\n",
            "Updated model weights saved to resnet_lr_0.0001_batch16.pth\n",
            "now BATCH is 16 and lr is 5e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.3700, Validation Accuracy: 0.8418, ROC AUC: 0.9267\n",
            "Updated model weights saved to resnet_lr_5e-05_batch16.pth\n",
            "now BATCH is 16 and lr is 1e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.3884, Validation Accuracy: 0.8262, ROC AUC: 0.9137\n",
            "Updated model weights saved to resnet_lr_1e-05_batch16.pth\n",
            "now BATCH is 32\n",
            "now BATCH is 32 and lr is 0.05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[69], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     17\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# Опционально\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mcontinue_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlearn_r\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mweights_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnew_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet_lr_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_batch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBATCH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[63], line 30\u001b[0m, in \u001b[0;36mcontinue_training\u001b[1;34m(train_loader, valid_loader, criterion, optimizer, model_name, model, scheduler, epochs, learn_r, weights_path, new_save_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:147\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    144\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    146\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m--> 147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#NEW_BATCH_SIZE=128\n",
        "#train_loader = prepare_dataloader(train_csv, train_dir, train_transforms, batch_size=NEW_BATCH_SIZE, shuffle=True)\n",
        "#valid_loader = prepare_dataloader(valid_csv, valid_dir, valid_transforms, batch_size=NEW_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for BATCH in [16, 32, 64, 128]:\n",
        "    print(f'now BATCH is {BATCH}')\n",
        "    FRACTION_OF_TRAIN_SET=0.45\n",
        "    train_loader = prepare_dataloader(train_csv, train_dir, train_transforms, batch_size=BATCH, shuffle=True)\n",
        "    train_loader_subset = get_subset_loader(train_loader, fraction=FRACTION_OF_TRAIN_SET)\n",
        "    valid_loader = prepare_dataloader(valid_csv, valid_dir, valid_transforms, batch_size=BATCH, shuffle=False)\n",
        "    for learning_rate in [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]:\n",
        "            print(f'now BATCH is {BATCH} and lr is {learning_rate}')\n",
        "            model = load_resnet_80_percent().to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Опционально\n",
        "            continue_training(train_loader=train_loader_subset,\n",
        "                              valid_loader=valid_loader,\n",
        "                              criterion=criterion,\n",
        "                              optimizer = optimizer,\n",
        "                              model_name=None,\n",
        "                              model=model,\n",
        "                              scheduler=scheduler,\n",
        "                              epochs=1,\n",
        "                              learn_r=learning_rate,\n",
        "                              weights_path=None,\n",
        "                              new_save_path=f'resnet_lr_{learning_rate}_batch{BATCH}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918b62e2-cd36-48e0-9517-3ff986319d46",
      "metadata": {
        "id": "918b62e2-cd36-48e0-9517-3ff986319d46",
        "outputId": "64f9dd8c-a9b6-4fd3-c6e0-a3bafbeb9e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr is 0.01, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.7331, Valid Loss: 0.6928, Accuracy: 0.5970\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.5970052083333334_valid_loss_0.6927940410872301\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.6722, Valid Loss: 0.6690, Accuracy: 0.6029\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.6028645833333334_valid_loss_0.669025640313824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Train Loss: 0.6654, Valid Loss: 0.6716, Accuracy: 0.6003\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.6002604166666666_valid_loss_0.6715954405566057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.6697, Valid Loss: 0.6926, Accuracy: 0.5970\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.5970052083333334_valid_loss_0.6925956370929877\n",
            "lr is 0.005, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.6775, Valid Loss: 0.8958, Accuracy: 0.5573\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.5572916666666666_valid_loss_0.8958195165420572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.6710, Valid Loss: 0.6929, Accuracy: 0.5853\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.5852864583333334_valid_loss_0.6929230087747177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Train Loss: 0.6674, Valid Loss: 0.6786, Accuracy: 0.5905\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.5904947916666666_valid_loss_0.6786416471004486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.6432, Valid Loss: 0.6418, Accuracy: 0.6204\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.6204427083333334_valid_loss_0.6418346123148998\n",
            "lr is 0.001, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.5475, Valid Loss: 0.6921, Accuracy: 0.6888\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.6888020833333334_valid_loss_0.6920803496614099\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.4960, Valid Loss: 0.6803, Accuracy: 0.6719\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.671875_valid_loss_0.6803151126950979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Train Loss: 0.4760, Valid Loss: 0.4772, Accuracy: 0.7734\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.7734375_valid_loss_0.4771711028491457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.4658, Valid Loss: 0.5085, Accuracy: 0.7552\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.7552083333333334_valid_loss_0.5084767742082477\n",
            "lr is 0.003, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.6587, Valid Loss: 0.6658, Accuracy: 0.6263\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.6263020833333334_valid_loss_0.6657711928710341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.6120, Valid Loss: 0.6344, Accuracy: 0.6530\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.6529947916666666_valid_loss_0.6344185822332898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Train Loss: 0.5943, Valid Loss: 0.6072, Accuracy: 0.6673\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.6673177083333334_valid_loss_0.6072102477774024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.5900, Valid Loss: 0.5962, Accuracy: 0.6738\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.673828125_valid_loss_0.5962317467977604\n",
            "lr is 0.0005, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.4411, Valid Loss: 0.4461, Accuracy: 0.8203\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8203125_valid_loss_0.44610250159166753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.4137, Valid Loss: 0.5070, Accuracy: 0.7682\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.7682291666666666_valid_loss_0.5069823008961976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Train Loss: 0.3991, Valid Loss: 0.3552, Accuracy: 0.8457\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.845703125_valid_loss_0.35516285492728156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.3881, Valid Loss: 0.4973, Accuracy: 0.7936\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.7936197916666666_valid_loss_0.4973223175232609\n",
            "Training stopped due to significant loss increase at Epoch 4\n",
            "lr is 0.0003, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.3952, Valid Loss: 0.3447, Accuracy: 0.8607\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8606770833333334_valid_loss_0.3447067642118782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.3404, Valid Loss: 0.3887, Accuracy: 0.8346\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8346354166666666_valid_loss_0.3886822290563335\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Train Loss: 0.3253, Valid Loss: 0.5234, Accuracy: 0.8060\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8059895833333334_valid_loss_0.5234181786266466\n",
            "Training stopped due to significant loss increase at Epoch 3\n",
            "lr is 0.0001, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.3400, Valid Loss: 0.3319, Accuracy: 0.8763\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8763020833333334_valid_loss_0.3318950613029301\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.2625, Valid Loss: 0.2868, Accuracy: 0.8945\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.89453125_valid_loss_0.28675911695851636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Train Loss: 0.2257, Valid Loss: 0.3000, Accuracy: 0.8926\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.892578125_valid_loss_0.2999666694086045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.2056, Valid Loss: 0.4017, Accuracy: 0.8542\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8541666666666666_valid_loss_0.40174557198770344\n",
            "Training stopped due to significant loss increase at Epoch 4\n",
            "lr is 5e-05, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.3399, Valid Loss: 0.2754, Accuracy: 0.8906\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.890625_valid_loss_0.27541186241433024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.2513, Valid Loss: 0.3088, Accuracy: 0.8796\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8795572916666666_valid_loss_0.3088111055549234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Train Loss: 0.2106, Valid Loss: 0.3207, Accuracy: 0.8913\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8912760416666666_valid_loss_0.3207009731559083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.1816, Valid Loss: 0.3532, Accuracy: 0.8874\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8873697916666666_valid_loss_0.35324579566561926\n",
            "lr is 3e-05, batch 16, im size is 224x224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Train Loss: 0.3734, Valid Loss: 0.3077, Accuracy: 0.8783\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8782552083333334_valid_loss_0.3076865029676507\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Train Loss: 0.2596, Valid Loss: 0.3354, Accuracy: 0.8750\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.875_valid_loss_0.33537155696346116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                       \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     21\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# Опционально\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m model, train_losses, valid_losses, valid_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50_batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_image_size\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_x\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_x\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_val_accur_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_accuracies\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[22], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, criterion, optimizer, epochs, scheduler, early_stopping_threshold)\u001b[0m\n\u001b[0;32m     18\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    144\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m--> 146\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for size_x in [224, 128]:\n",
        "    train_transforms = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.Rotate(limit=25),\n",
        "        A.RandomResizedCrop(height=size_x, width=size_x, scale=(0.9, 0.9)),\n",
        "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    for BATCH in [16, 32, 64, 128, 224]:\n",
        "        train_loader = prepare_dataloader(train_csv, train_dir, train_transforms, batch_size=BATCH, shuffle=True)\n",
        "        valid_loader = prepare_dataloader(valid_csv, valid_dir, valid_transforms, batch_size=BATCH, shuffle=False)\n",
        "\n",
        "        for learning_rate in [0.00005, 0.00003, 0.00001, 0.000005, 0.000003, 0.000001]:\n",
        "            print(f'lr is {learning_rate}, batch {BATCH}, im size is {size_x}x{size_x}')\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            model = get_pretrained_resnet50(num_classes=2).to(device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Опционально\n",
        "            model, train_losses, valid_losses, valid_accuracies = train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=4, scheduler=scheduler)\n",
        "            torch.save(model.state_dict(), f'resnet50_batch_{BATCH}_image_size{size_x}x{size_x}_lr_{learning_rate}_val_accur_{valid_accuracies}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a48666-a154-40db-9a67-473f5b60f600",
      "metadata": {
        "id": "95a48666-a154-40db-9a67-473f5b60f600"
      },
      "outputs": [],
      "source": [
        "#NEW_BATCH_SIZE=128\n",
        "#train_loader = prepare_dataloader(train_csv, train_dir, train_transforms, batch_size=NEW_BATCH_SIZE, shuffle=True)\n",
        "#valid_loader = prepare_dataloader(valid_csv, valid_dir, valid_transforms, batch_size=NEW_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for BATCH in [16, 32, 64]:\n",
        "    print(f'now BATCH is {BATCH}')\n",
        "    FRACTION_OF_TRAIN_SET=0.45\n",
        "    train_loader = prepare_dataloader(train_csv, train_dir, train_transforms, batch_size=BATCH, shuffle=True)\n",
        "    train_loader_subset = get_subset_loader(train_loader, fraction=FRACTION_OF_TRAIN_SET)\n",
        "    valid_loader = prepare_dataloader(valid_csv, valid_dir, valid_transforms, batch_size=BATCH, shuffle=False)\n",
        "    for learning_rate in [0.00005, 0.00003, 0.00001, 0.000005, 0.000003, 0.000001, 0.0000005, 0.0000003, 0.0000001]:\n",
        "            print(f'now BATCH is {BATCH} and lr is {learning_rate}')\n",
        "            model = load_resnet_80_percent().to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Опционально\n",
        "            continue_training(train_loader=train_loader_subset,\n",
        "                              valid_loader=valid_loader,\n",
        "                              criterion=criterion,\n",
        "                              optimizer = optimizer,\n",
        "                              model_name=None,\n",
        "                              model=model,\n",
        "                              scheduler=scheduler,\n",
        "                              epochs=1,\n",
        "                              learn_r=learning_rate,\n",
        "                              weights_path=None,\n",
        "                              new_save_path=f'resnet_lr_{learning_rate}_batch{BATCH}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1ec63f-e152-4e2b-bc33-45583aec8f2c",
      "metadata": {
        "id": "0c1ec63f-e152-4e2b-bc33-45583aec8f2c",
        "outputId": "4cb53199-87b0-49a2-99af-d1a0af539e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "now BATCH is 64\n",
            "now BATCH is 64 and lr is 1e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.2736, Validation Accuracy: 0.8939, ROC AUC: 0.9552\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8938802083333334_valid_loss_6.566490292549133\n",
            "Updated model weights saved to resnet_lr_1e-07_batch64.pth\n",
            "now BATCH is 64 and lr is 3e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.2788, Validation Accuracy: 0.8965, ROC AUC: 0.9546\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.896484375_valid_loss_6.692318812012672\n",
            "Updated model weights saved to resnet_lr_3e-07_batch64.pth\n",
            "now BATCH is 64 and lr is 5e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.2729, Validation Accuracy: 0.8971, ROC AUC: 0.9555\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8971354166666666_valid_loss_6.550626754760742\n",
            "Updated model weights saved to resnet_lr_5e-07_batch64.pth\n",
            "now BATCH is 64 and lr is 1e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.2719, Validation Accuracy: 0.8952, ROC AUC: 0.9558\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8951822916666666_valid_loss_6.525370255112648\n",
            "Updated model weights saved to resnet_lr_1e-06_batch64.pth\n",
            "now BATCH is 64 and lr is 3e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Validation Loss: 0.2766, Validation Accuracy: 0.8952, ROC AUC: 0.9554\n",
            "Updated model weights saved to resnet_50_weights_for_accuracy_0.8951822916666666_valid_loss_6.639599561691284\n",
            "Updated model weights saved to resnet_lr_3e-06_batch64.pth\n",
            "now BATCH is 64 and lr is 5e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Мой компьютер\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "                                                                                                                       \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     13\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# Опционально\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mcontinue_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlearn_r\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mweights_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnew_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet_lr_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_batch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBATCH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[31], line 32\u001b[0m, in \u001b[0;36mcontinue_training\u001b[1;34m(train_loader, valid_loader, criterion, optimizer, model_name, model, scheduler, epochs, learn_r, weights_path, new_save_path)\u001b[0m\n\u001b[0;32m     30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for BATCH in [64, 32, 16]:\n",
        "    print(f'now BATCH is {BATCH}')\n",
        "    FRACTION_OF_TRAIN_SET=0.45\n",
        "    train_loader = prepare_dataloader(train_csv, train_dir, train_transforms, batch_size=BATCH, shuffle=True)\n",
        "    train_loader_subset = get_subset_loader(train_loader, fraction=FRACTION_OF_TRAIN_SET)\n",
        "    valid_loader = prepare_dataloader(valid_csv, valid_dir, valid_transforms, batch_size=BATCH, shuffle=False)\n",
        "    for learning_rate in reversed([0.05, 0.03, 0.01, 0.005, 0.003, 0.001, 0.0005, 0.0003, 0.0001, 0.00005, 0.00003, 0.00001, 0.000005, 0.000003, 0.000001, 0.0000005, 0.0000003, 0.0000001]):\n",
        "            print(f'now BATCH is {BATCH} and lr is {learning_rate}')\n",
        "            model = load_resnet_80_percent().to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Опционально\n",
        "            continue_training(train_loader=train_loader_subset,\n",
        "                              valid_loader=valid_loader,\n",
        "                              criterion=criterion,\n",
        "                              optimizer = optimizer,\n",
        "                              model_name=None,\n",
        "                              model=model,\n",
        "                              scheduler=scheduler,\n",
        "                              epochs=1,\n",
        "                              learn_r=learning_rate,\n",
        "                              weights_path=None,\n",
        "                              new_save_path=f'resnet_lr_{learning_rate}_batch{BATCH}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a4fddaa-b90b-4f94-989e-db5f13b79867",
      "metadata": {
        "id": "1a4fddaa-b90b-4f94-989e-db5f13b79867"
      },
      "outputs": [],
      "source": [
        "def scheduler(optimizer, now_epoch, initial_lr, epochs):\n",
        "    end_lr_rate = 0.01  # end_lr = initial_lr * end_lr_rate\n",
        "    rate = ((1 + math.cos(now_epoch * math.pi / epochs)) / 2) * (1 - end_lr_rate) + end_lr_rate\n",
        "    new_lr = rate * initial_lr\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "    return new_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc59ef5b-1742-423c-8d87-83edf572bbaa",
      "metadata": {
        "id": "fc59ef5b-1742-423c-8d87-83edf572bbaa"
      },
      "outputs": [],
      "source": [
        "train_transforms = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.Rotate(limit=25),\n",
        "        A.RandomResizedCrop(height=224, width=224, scale=(0.9, 0.9)),\n",
        "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4fe0210-4260-407f-9fcf-097780138513",
      "metadata": {
        "id": "b4fe0210-4260-407f-9fcf-097780138513",
        "outputId": "9d0d704b-7381-401e-9453-96c2c9d64e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Batch size: 128, Labels size: 128, Probabilities size: 128\n",
            "Validation Accuracy: 0.8971354166666666\n",
            "ROC AUC Score: 0.9554721523917431\n",
            "Sample Probabilities for Class 1: [0.95841193 0.01654959 0.02925532 0.10077992 0.0406514 ]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from torchvision import models\n",
        "def load_resnet_best_percent(path=None):\n",
        "    model = models.resnet50(weights=None)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, 2)\n",
        "    if not path:\n",
        "        model.load_state_dict(torch.load('resnet_50_w_897_perc', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
        "    return model\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = load_resnet_best_percent()\n",
        "model.eval()\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        probabilities = nn.Softmax(dim=1)(outputs.clone().detach())\n",
        "\n",
        "        probs_class_1 = probabilities[:, 1].cpu().numpy()\n",
        "        all_probs.extend(probs_class_1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print(f'Batch size: {inputs.size(0)}, Labels size: {labels.size(0)}, Probabilities size: {probs_class_1.size}')\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_probs = np.array(all_probs)\n",
        "\n",
        "valid_accuracy = accuracy_score(all_labels, (all_probs > 0.5).astype(int))\n",
        "roc_auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "print(\"Validation Accuracy:\", valid_accuracy)\n",
        "print(\"ROC AUC Score:\", roc_auc)\n",
        "print(\"Sample Probabilities for Class 1:\", all_probs[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "598efb99-a133-4141-954c-1f4d718dabd3",
      "metadata": {
        "id": "598efb99-a133-4141-954c-1f4d718dabd3",
        "outputId": "be48fc38-9f0f-4d5e-8908-5ad5c14b056d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid1.jpg, prob: 0.9584119319915771\n",
            "valid2.jpg, prob: 0.016549590975046158\n",
            "valid3.jpg, prob: 0.02925531566143036\n",
            "valid4.jpg, prob: 0.10077992081642151\n",
            "valid5.jpg, prob: 0.04065139591693878\n",
            "valid6.jpg, prob: 0.8007073998451233\n",
            "valid7.jpg, prob: 0.049834027886390686\n",
            "valid8.jpg, prob: 0.05276158079504967\n",
            "valid9.jpg, prob: 0.0005217966972850263\n",
            "valid10.jpg, prob: 0.5282792448997498\n",
            "valid11.jpg, prob: 0.9562975764274597\n",
            "valid12.jpg, prob: 0.9708375334739685\n",
            "valid13.jpg, prob: 0.016439098864793777\n",
            "valid14.jpg, prob: 0.9206658005714417\n",
            "valid15.jpg, prob: 0.9996880292892456\n",
            "valid16.jpg, prob: 0.21216605603694916\n",
            "valid17.jpg, prob: 0.999099612236023\n",
            "valid18.jpg, prob: 0.0017090424662455916\n",
            "valid19.jpg, prob: 0.5621709227561951\n",
            "valid20.jpg, prob: 0.9995854496955872\n",
            "valid21.jpg, prob: 0.9304478764533997\n",
            "valid22.jpg, prob: 0.1816292554140091\n",
            "valid23.jpg, prob: 0.002914269920438528\n",
            "valid24.jpg, prob: 0.01911434344947338\n",
            "valid25.jpg, prob: 0.9950626492500305\n",
            "valid26.jpg, prob: 0.56449294090271\n",
            "valid27.jpg, prob: 0.864176869392395\n",
            "valid28.jpg, prob: 0.30034372210502625\n",
            "valid29.jpg, prob: 0.006847969256341457\n",
            "valid30.jpg, prob: 0.958341121673584\n",
            "valid31.jpg, prob: 0.9864274859428406\n",
            "valid32.jpg, prob: 0.00011876544158440083\n",
            "valid33.jpg, prob: 0.27483218908309937\n",
            "valid34.jpg, prob: 0.10794159770011902\n",
            "valid35.jpg, prob: 0.002401835983619094\n",
            "valid36.jpg, prob: 0.9971465468406677\n",
            "valid37.jpg, prob: 9.055456757778302e-05\n",
            "valid38.jpg, prob: 0.4727749526500702\n",
            "valid39.jpg, prob: 0.041783370077610016\n",
            "valid40.jpg, prob: 0.9723224639892578\n",
            "valid41.jpg, prob: 0.9896523356437683\n",
            "valid42.jpg, prob: 0.0004239105910528451\n",
            "valid43.jpg, prob: 0.0022227545268833637\n",
            "valid44.jpg, prob: 0.8987858891487122\n",
            "valid45.jpg, prob: 0.5907719135284424\n",
            "valid46.jpg, prob: 0.0010555337648838758\n",
            "valid47.jpg, prob: 0.9973209500312805\n",
            "valid48.jpg, prob: 0.9834110140800476\n",
            "valid49.jpg, prob: 0.00025063511566258967\n",
            "valid50.jpg, prob: 0.7734276652336121\n",
            "valid51.jpg, prob: 0.12942075729370117\n",
            "valid52.jpg, prob: 0.9399226903915405\n",
            "valid53.jpg, prob: 0.004484670236706734\n",
            "valid54.jpg, prob: 0.013435622677206993\n",
            "valid55.jpg, prob: 0.9733275771141052\n",
            "valid56.jpg, prob: 0.12963701784610748\n",
            "valid57.jpg, prob: 0.019507842138409615\n",
            "valid58.jpg, prob: 0.0066642798483371735\n",
            "valid59.jpg, prob: 0.002543435199186206\n",
            "valid60.jpg, prob: 0.0149579718708992\n",
            "valid61.jpg, prob: 0.0575413815677166\n",
            "valid62.jpg, prob: 0.37337884306907654\n",
            "valid63.jpg, prob: 0.9785274863243103\n",
            "valid64.jpg, prob: 0.006290322169661522\n",
            "valid65.jpg, prob: 0.03569711744785309\n",
            "valid66.jpg, prob: 0.05824767425656319\n",
            "valid67.jpg, prob: 0.22101974487304688\n",
            "valid68.jpg, prob: 0.0001639104593778029\n",
            "valid69.jpg, prob: 0.2629300355911255\n",
            "valid70.jpg, prob: 0.22358442842960358\n",
            "valid71.jpg, prob: 0.03155285492539406\n",
            "valid72.jpg, prob: 0.7419154644012451\n",
            "valid73.jpg, prob: 0.9867368340492249\n",
            "valid74.jpg, prob: 0.9952424764633179\n",
            "valid75.jpg, prob: 0.0027038545813411474\n",
            "valid76.jpg, prob: 0.015999091789126396\n",
            "valid77.jpg, prob: 0.9836773872375488\n",
            "valid78.jpg, prob: 0.0030227666720747948\n",
            "valid79.jpg, prob: 0.33473801612854004\n",
            "valid80.jpg, prob: 0.013320409692823887\n",
            "valid81.jpg, prob: 0.9847816228866577\n",
            "valid82.jpg, prob: 0.003759654937312007\n",
            "valid83.jpg, prob: 0.40708127617836\n",
            "valid84.jpg, prob: 0.04211810976266861\n",
            "valid85.jpg, prob: 0.8574191331863403\n",
            "valid86.jpg, prob: 0.23210255801677704\n",
            "valid87.jpg, prob: 0.0031032366678118706\n",
            "valid88.jpg, prob: 0.20332838594913483\n",
            "valid89.jpg, prob: 0.9772456288337708\n",
            "valid90.jpg, prob: 0.07669977098703384\n",
            "valid91.jpg, prob: 0.9988285899162292\n",
            "valid92.jpg, prob: 0.00097182288300246\n",
            "valid93.jpg, prob: 0.000995539128780365\n",
            "valid94.jpg, prob: 0.0033861156553030014\n",
            "valid95.jpg, prob: 0.08399541676044464\n",
            "valid96.jpg, prob: 0.24640780687332153\n",
            "valid97.jpg, prob: 0.01735413447022438\n",
            "valid98.jpg, prob: 0.013358265161514282\n",
            "valid99.jpg, prob: 0.03899519518017769\n",
            "valid100.jpg, prob: 0.9969300627708435\n",
            "valid101.jpg, prob: 0.4754489064216614\n",
            "valid102.jpg, prob: 0.9992165565490723\n",
            "valid103.jpg, prob: 0.09216301888227463\n",
            "valid104.jpg, prob: 0.0821332186460495\n",
            "valid105.jpg, prob: 0.003713355166837573\n",
            "valid106.jpg, prob: 0.004749924410134554\n",
            "valid107.jpg, prob: 0.8250592350959778\n",
            "valid108.jpg, prob: 0.20205602049827576\n",
            "valid109.jpg, prob: 0.5537701845169067\n",
            "valid110.jpg, prob: 0.9678978323936462\n",
            "valid111.jpg, prob: 0.14408990740776062\n",
            "valid112.jpg, prob: 0.0037738014943897724\n",
            "valid113.jpg, prob: 0.0052880821749567986\n",
            "valid114.jpg, prob: 0.9988962411880493\n",
            "valid115.jpg, prob: 0.9976709485054016\n",
            "valid116.jpg, prob: 0.05942654609680176\n",
            "valid117.jpg, prob: 0.0002317063044756651\n",
            "valid118.jpg, prob: 0.0904863029718399\n",
            "valid119.jpg, prob: 0.15656861662864685\n",
            "valid120.jpg, prob: 0.8911580443382263\n",
            "valid121.jpg, prob: 0.0016549399588257074\n",
            "valid122.jpg, prob: 0.0013556017074733973\n",
            "valid123.jpg, prob: 0.006374394986778498\n",
            "valid124.jpg, prob: 0.8805570006370544\n",
            "valid125.jpg, prob: 0.00010398244194220752\n",
            "valid126.jpg, prob: 0.00813165120780468\n",
            "valid127.jpg, prob: 0.2618727385997772\n",
            "valid128.jpg, prob: 0.09131984412670135\n",
            "valid129.jpg, prob: 0.86663419008255\n",
            "valid130.jpg, prob: 0.006365032866597176\n",
            "valid131.jpg, prob: 0.8588681221008301\n",
            "valid132.jpg, prob: 0.12239564210176468\n",
            "valid133.jpg, prob: 0.01672467216849327\n",
            "valid134.jpg, prob: 0.9278011322021484\n",
            "valid135.jpg, prob: 0.13199645280838013\n",
            "valid136.jpg, prob: 0.09009642899036407\n",
            "valid137.jpg, prob: 0.006832618732005358\n",
            "valid138.jpg, prob: 0.027500994503498077\n",
            "valid139.jpg, prob: 0.8466959595680237\n",
            "valid140.jpg, prob: 0.008070064708590508\n",
            "valid141.jpg, prob: 0.0033946356270462275\n",
            "valid142.jpg, prob: 0.0022200150415301323\n",
            "valid143.jpg, prob: 0.9382777214050293\n",
            "valid144.jpg, prob: 0.9931372404098511\n",
            "valid145.jpg, prob: 0.06792999058961868\n",
            "valid146.jpg, prob: 0.1584467738866806\n",
            "valid147.jpg, prob: 0.6018854975700378\n",
            "valid148.jpg, prob: 0.0951298400759697\n",
            "valid149.jpg, prob: 0.9668174386024475\n",
            "valid150.jpg, prob: 0.015737434849143028\n",
            "valid151.jpg, prob: 0.021907519549131393\n",
            "valid152.jpg, prob: 0.9921135902404785\n",
            "valid153.jpg, prob: 0.9755767583847046\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, int(len(all_probs)*0.1)):\n",
        "    print(f'valid{i+1}.jpg, prob: {all_probs[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160b3815-e458-457d-86ad-f26d01107776",
      "metadata": {
        "id": "160b3815-e458-457d-86ad-f26d01107776"
      },
      "outputs": [],
      "source": [
        "test_transforms = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "print('test data transforms loaded')\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_dir, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)[\"image\"]\n",
        "\n",
        "        return image, self.image_files[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d57b4d3-ec48-4379-8da6-c353643012ce",
      "metadata": {
        "id": "0d57b4d3-ec48-4379-8da6-c353643012ce"
      },
      "outputs": [],
      "source": [
        "model = load_resnet_best_percent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fece4dc1-76c6-4450-b86c-ee56da5468cb",
      "metadata": {
        "id": "fece4dc1-76c6-4450-b86c-ee56da5468cb"
      },
      "outputs": [],
      "source": [
        "test_transforms = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_dir, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)[\"image\"]\n",
        "\n",
        "        return image, self.image_files[idx]\n",
        "test_dir = 'test'\n",
        "test_dataset = TestDataset(test_dir, transforms=test_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08951d47-6b2f-45e2-85aa-ae4cd7813d7b",
      "metadata": {
        "id": "08951d47-6b2f-45e2-85aa-ae4cd7813d7b",
        "outputId": "aaa88ae9-221c-4c20-ea77-6bf2e4f951cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predictions made\n",
            "test1.jpg, prob: 0.017498066648840904\n",
            "test2.jpg, prob: 0.0015255126636475325\n",
            "test3.jpg, prob: 0.08327808976173401\n",
            "test4.jpg, prob: 0.49012696743011475\n",
            "test5.jpg, prob: 0.00018353310588281602\n",
            "test6.jpg, prob: 0.0007997814100235701\n",
            "test7.jpg, prob: 0.5294589996337891\n",
            "test8.jpg, prob: 0.9882142543792725\n",
            "test9.jpg, prob: 0.012590080499649048\n",
            "test10.jpg, prob: 0.019112441688776016\n",
            "test11.jpg, prob: 0.8994684219360352\n",
            "test12.jpg, prob: 0.06712892651557922\n",
            "test13.jpg, prob: 0.0010266819735988975\n",
            "test14.jpg, prob: 0.05527040362358093\n",
            "test15.jpg, prob: 0.17593280971050262\n",
            "test16.jpg, prob: 0.07931371033191681\n",
            "test17.jpg, prob: 0.996394693851471\n",
            "test18.jpg, prob: 0.9937232136726379\n",
            "test19.jpg, prob: 0.8809841871261597\n",
            "test20.jpg, prob: 0.2748587429523468\n",
            "test21.jpg, prob: 0.002357499673962593\n",
            "test22.jpg, prob: 0.9579718708992004\n",
            "test23.jpg, prob: 0.1676693856716156\n",
            "test24.jpg, prob: 0.7842541337013245\n",
            "test25.jpg, prob: 0.5094687342643738\n",
            "test26.jpg, prob: 0.039665255695581436\n",
            "test27.jpg, prob: 0.019595764577388763\n",
            "test28.jpg, prob: 0.14228671789169312\n",
            "test29.jpg, prob: 0.0005159596330486238\n",
            "test30.jpg, prob: 0.11607114225625992\n",
            "test31.jpg, prob: 0.15124304592609406\n",
            "test32.jpg, prob: 0.04538361728191376\n",
            "test33.jpg, prob: 0.9931938052177429\n",
            "test34.jpg, prob: 0.01387186348438263\n",
            "test35.jpg, prob: 0.9905816316604614\n",
            "test36.jpg, prob: 0.03695295751094818\n",
            "test37.jpg, prob: 0.1137109100818634\n",
            "test38.jpg, prob: 0.0023376280441880226\n",
            "test39.jpg, prob: 0.0005400952650234103\n",
            "test40.jpg, prob: 0.17995768785476685\n",
            "test41.jpg, prob: 0.9963343143463135\n",
            "test42.jpg, prob: 0.9637646079063416\n",
            "test43.jpg, prob: 0.045099690556526184\n",
            "test44.jpg, prob: 0.9939172863960266\n",
            "test45.jpg, prob: 0.998908519744873\n",
            "test46.jpg, prob: 0.5907537937164307\n",
            "test47.jpg, prob: 0.9862048029899597\n",
            "test48.jpg, prob: 0.03580550476908684\n",
            "test49.jpg, prob: 0.001057117828167975\n",
            "test50.jpg, prob: 0.9982315897941589\n",
            "test51.jpg, prob: 0.05051941052079201\n",
            "test52.jpg, prob: 0.0011312541319057345\n",
            "test53.jpg, prob: 0.0006768220337107778\n",
            "test54.jpg, prob: 0.9634644985198975\n",
            "test55.jpg, prob: 0.002467671874910593\n",
            "test56.jpg, prob: 0.951335072517395\n",
            "test57.jpg, prob: 0.004201008938252926\n",
            "test58.jpg, prob: 0.46811050176620483\n",
            "test59.jpg, prob: 0.9981315732002258\n",
            "test60.jpg, prob: 0.005692127160727978\n",
            "test61.jpg, prob: 0.001444823108613491\n",
            "test62.jpg, prob: 0.0003714909835252911\n",
            "test63.jpg, prob: 0.09063910692930222\n",
            "test64.jpg, prob: 0.030760541558265686\n",
            "test65.jpg, prob: 0.000732655928004533\n",
            "test66.jpg, prob: 0.01109638623893261\n",
            "test67.jpg, prob: 0.5206056833267212\n",
            "test68.jpg, prob: 0.9631935954093933\n",
            "test69.jpg, prob: 0.000236545703955926\n",
            "test70.jpg, prob: 0.28727269172668457\n",
            "test71.jpg, prob: 0.027071474120020866\n",
            "test72.jpg, prob: 0.3992759883403778\n",
            "test73.jpg, prob: 0.9936268329620361\n",
            "test74.jpg, prob: 0.025998499244451523\n",
            "test75.jpg, prob: 0.10507973283529282\n",
            "test76.jpg, prob: 0.0010423362255096436\n",
            "test77.jpg, prob: 0.009406642988324165\n",
            "test78.jpg, prob: 0.00024577806470915675\n",
            "test79.jpg, prob: 0.013663549907505512\n",
            "test80.jpg, prob: 0.9739477634429932\n",
            "test81.jpg, prob: 0.9990257024765015\n",
            "test82.jpg, prob: 0.000990265398286283\n",
            "test83.jpg, prob: 0.940370500087738\n",
            "test84.jpg, prob: 0.004040233790874481\n",
            "test85.jpg, prob: 0.15457089245319366\n",
            "test86.jpg, prob: 0.11144182085990906\n",
            "test87.jpg, prob: 0.07228882610797882\n",
            "test88.jpg, prob: 0.005857374984771013\n",
            "test89.jpg, prob: 0.5273321270942688\n",
            "test90.jpg, prob: 0.02346685342490673\n",
            "test91.jpg, prob: 0.9989431500434875\n",
            "test92.jpg, prob: 0.002343402011319995\n",
            "test93.jpg, prob: 0.00032267606002278626\n",
            "test94.jpg, prob: 0.0010988059220835567\n",
            "test95.jpg, prob: 0.1072053462266922\n",
            "test96.jpg, prob: 0.9958227872848511\n",
            "test97.jpg, prob: 0.0018992065452039242\n",
            "test98.jpg, prob: 0.000389259890653193\n",
            "test99.jpg, prob: 0.019501227885484695\n",
            "test100.jpg, prob: 0.9946061968803406\n",
            "test101.jpg, prob: 0.9222800135612488\n",
            "test102.jpg, prob: 0.01787429489195347\n",
            "test103.jpg, prob: 0.4556637108325958\n",
            "test104.jpg, prob: 0.933001697063446\n",
            "test105.jpg, prob: 0.24632704257965088\n",
            "test106.jpg, prob: 0.03146436810493469\n",
            "test107.jpg, prob: 0.0004267321201041341\n",
            "test108.jpg, prob: 0.9935124516487122\n",
            "test109.jpg, prob: 5.008881271351129e-05\n",
            "test110.jpg, prob: 0.9937625527381897\n",
            "test111.jpg, prob: 0.02645188756287098\n",
            "test112.jpg, prob: 0.0032313831616193056\n",
            "test113.jpg, prob: 0.6502918601036072\n",
            "test114.jpg, prob: 0.024308132007718086\n",
            "test115.jpg, prob: 0.051312096416950226\n",
            "test116.jpg, prob: 0.1071038767695427\n",
            "test117.jpg, prob: 0.7370579242706299\n",
            "test118.jpg, prob: 0.05119872838258743\n",
            "test119.jpg, prob: 0.8207338452339172\n",
            "test120.jpg, prob: 0.9865626692771912\n",
            "test121.jpg, prob: 0.03191351518034935\n",
            "test122.jpg, prob: 0.9915224313735962\n",
            "test123.jpg, prob: 0.020953115075826645\n",
            "test124.jpg, prob: 0.0029221007134765387\n",
            "test125.jpg, prob: 0.04518396407365799\n",
            "test126.jpg, prob: 0.023372625932097435\n",
            "test127.jpg, prob: 0.9993459582328796\n",
            "test128.jpg, prob: 0.9976231455802917\n",
            "test129.jpg, prob: 0.3635696768760681\n",
            "test130.jpg, prob: 0.017999161034822464\n",
            "test131.jpg, prob: 0.9987046718597412\n",
            "test132.jpg, prob: 0.0897592231631279\n",
            "test133.jpg, prob: 0.007023464888334274\n",
            "test134.jpg, prob: 0.00039661512710154057\n",
            "test135.jpg, prob: 0.9998251795768738\n",
            "test136.jpg, prob: 0.9834360480308533\n",
            "test137.jpg, prob: 0.9966310858726501\n",
            "test138.jpg, prob: 0.012257920578122139\n",
            "test139.jpg, prob: 0.00982038863003254\n",
            "test140.jpg, prob: 0.028613729402422905\n",
            "test141.jpg, prob: 0.0056160083040595055\n",
            "test142.jpg, prob: 0.01841193437576294\n",
            "test143.jpg, prob: 0.04417301341891289\n",
            "test144.jpg, prob: 0.0016960524953901768\n",
            "test145.jpg, prob: 0.6505974531173706\n",
            "test146.jpg, prob: 0.021633649244904518\n",
            "test147.jpg, prob: 0.0004390356771182269\n",
            "test148.jpg, prob: 0.009506470523774624\n",
            "test149.jpg, prob: 0.24094951152801514\n",
            "test150.jpg, prob: 0.0878404825925827\n",
            "test151.jpg, prob: 0.031037921085953712\n",
            "test152.jpg, prob: 0.11262606084346771\n",
            "test153.jpg, prob: 0.0003864670288749039\n",
            "test154.jpg, prob: 0.0036934565287083387\n",
            "test155.jpg, prob: 0.028710749000310898\n",
            "test156.jpg, prob: 0.06465642899274826\n",
            "test157.jpg, prob: 0.9989567995071411\n",
            "test158.jpg, prob: 0.0013646510196849704\n",
            "test159.jpg, prob: 0.9861598610877991\n",
            "test160.jpg, prob: 0.2906453013420105\n",
            "test161.jpg, prob: 0.9672831296920776\n",
            "test162.jpg, prob: 0.0009046217310242355\n",
            "test163.jpg, prob: 0.9769968390464783\n",
            "test164.jpg, prob: 5.9973022871417925e-05\n",
            "test165.jpg, prob: 0.010867134667932987\n",
            "test166.jpg, prob: 0.21694467961788177\n",
            "test167.jpg, prob: 0.14131854474544525\n",
            "test168.jpg, prob: 0.010711691342294216\n",
            "test169.jpg, prob: 0.9971134662628174\n",
            "test170.jpg, prob: 0.04138612002134323\n",
            "test171.jpg, prob: 0.08112368732690811\n",
            "test172.jpg, prob: 0.048697587102651596\n",
            "test173.jpg, prob: 0.04121575877070427\n",
            "test174.jpg, prob: 0.997809112071991\n",
            "test175.jpg, prob: 0.2938421368598938\n",
            "test176.jpg, prob: 0.003990867640823126\n",
            "test177.jpg, prob: 0.933147132396698\n",
            "test178.jpg, prob: 0.6528875231742859\n",
            "test179.jpg, prob: 0.9540863633155823\n",
            "test180.jpg, prob: 0.012650253251194954\n",
            "test181.jpg, prob: 3.683748946059495e-05\n",
            "test182.jpg, prob: 0.00281774764880538\n",
            "test183.jpg, prob: 0.9688956141471863\n",
            "test184.jpg, prob: 0.004832381382584572\n",
            "test185.jpg, prob: 0.9310938119888306\n",
            "test186.jpg, prob: 0.2708204984664917\n",
            "test187.jpg, prob: 0.06934791803359985\n",
            "test188.jpg, prob: 0.3562312126159668\n",
            "test189.jpg, prob: 0.005031955428421497\n",
            "test190.jpg, prob: 0.9998372793197632\n",
            "test191.jpg, prob: 0.06754308193922043\n",
            "test192.jpg, prob: 0.3097105026245117\n",
            "test193.jpg, prob: 0.9351601004600525\n",
            "test194.jpg, prob: 0.037863630801439285\n",
            "test195.jpg, prob: 0.13656675815582275\n",
            "test196.jpg, prob: 0.00033714110031723976\n",
            "test197.jpg, prob: 0.03463180363178253\n",
            "test198.jpg, prob: 0.11586476862430573\n",
            "test199.jpg, prob: 0.008685259148478508\n",
            "test200.jpg, prob: 0.8655030727386475\n",
            "test201.jpg, prob: 0.014507001265883446\n",
            "test202.jpg, prob: 0.12500296533107758\n",
            "test203.jpg, prob: 0.00044375829747878015\n",
            "test204.jpg, prob: 0.02769106812775135\n",
            "test205.jpg, prob: 0.10634143650531769\n",
            "test206.jpg, prob: 0.0104216318577528\n",
            "test207.jpg, prob: 0.7521694302558899\n",
            "test208.jpg, prob: 0.031082412227988243\n",
            "test209.jpg, prob: 0.11419761925935745\n",
            "test210.jpg, prob: 0.006873389706015587\n",
            "test211.jpg, prob: 0.12011679261922836\n",
            "test212.jpg, prob: 0.00041270567453466356\n",
            "test213.jpg, prob: 0.0026715255808085203\n",
            "test214.jpg, prob: 0.005129138007760048\n",
            "test215.jpg, prob: 0.9972334504127502\n",
            "test216.jpg, prob: 0.9980815649032593\n",
            "test217.jpg, prob: 0.06397639960050583\n",
            "test218.jpg, prob: 0.06850939244031906\n",
            "test219.jpg, prob: 0.010621177963912487\n",
            "test220.jpg, prob: 0.0013720955466851592\n",
            "test221.jpg, prob: 0.596877932548523\n",
            "test222.jpg, prob: 0.003076753346249461\n",
            "test223.jpg, prob: 0.994778037071228\n",
            "test224.jpg, prob: 0.9963560104370117\n",
            "test225.jpg, prob: 0.024926230311393738\n",
            "test226.jpg, prob: 0.05716882646083832\n",
            "test227.jpg, prob: 0.9380191564559937\n",
            "test228.jpg, prob: 0.19297091662883759\n",
            "test229.jpg, prob: 0.005348971113562584\n",
            "test230.jpg, prob: 0.009957912378013134\n",
            "test231.jpg, prob: 0.46086734533309937\n",
            "test232.jpg, prob: 0.003434646874666214\n",
            "test233.jpg, prob: 0.022681228816509247\n",
            "test234.jpg, prob: 0.05268403887748718\n",
            "test235.jpg, prob: 0.02319212630391121\n",
            "test236.jpg, prob: 0.9814792275428772\n",
            "test237.jpg, prob: 0.033512599766254425\n",
            "test238.jpg, prob: 0.009902664460241795\n",
            "test239.jpg, prob: 0.0008336154278367758\n",
            "test240.jpg, prob: 0.006636679172515869\n",
            "test241.jpg, prob: 0.00262327934615314\n",
            "test242.jpg, prob: 0.0007494328892789781\n",
            "test243.jpg, prob: 0.02678763121366501\n",
            "test244.jpg, prob: 0.7442058324813843\n",
            "test245.jpg, prob: 0.034840378910303116\n",
            "test246.jpg, prob: 0.09916101396083832\n",
            "test247.jpg, prob: 0.0024279961362481117\n",
            "test248.jpg, prob: 0.12825535237789154\n",
            "test249.jpg, prob: 0.0006335406797006726\n",
            "test250.jpg, prob: 0.9983238577842712\n",
            "test251.jpg, prob: 0.322485089302063\n",
            "test252.jpg, prob: 0.012469015084207058\n",
            "test253.jpg, prob: 0.9774624109268188\n",
            "test254.jpg, prob: 0.9879580736160278\n",
            "test255.jpg, prob: 0.9912763833999634\n",
            "test256.jpg, prob: 0.004359059035778046\n",
            "test257.jpg, prob: 0.027635321021080017\n",
            "test258.jpg, prob: 0.9987996816635132\n",
            "test259.jpg, prob: 0.0030371914617717266\n",
            "test260.jpg, prob: 0.024424312636256218\n",
            "test261.jpg, prob: 0.15744172036647797\n",
            "test262.jpg, prob: 0.5169865489006042\n",
            "test263.jpg, prob: 0.8238800168037415\n",
            "test264.jpg, prob: 0.9356324076652527\n",
            "test265.jpg, prob: 0.0031899483874440193\n",
            "test266.jpg, prob: 0.08009656518697739\n",
            "test267.jpg, prob: 0.16415101289749146\n",
            "test268.jpg, prob: 0.46577680110931396\n",
            "test269.jpg, prob: 0.6192894577980042\n",
            "test270.jpg, prob: 0.003181166248396039\n",
            "test271.jpg, prob: 0.02258395403623581\n",
            "test272.jpg, prob: 0.009123576804995537\n",
            "test273.jpg, prob: 0.00044924154644832015\n",
            "test274.jpg, prob: 0.2606355845928192\n",
            "test275.jpg, prob: 0.9957965612411499\n",
            "test276.jpg, prob: 0.36369550228118896\n",
            "test277.jpg, prob: 0.16838319599628448\n",
            "test278.jpg, prob: 0.031901683658361435\n",
            "test279.jpg, prob: 0.0021039594430476427\n",
            "test280.jpg, prob: 0.9957626461982727\n",
            "test281.jpg, prob: 0.8687798380851746\n",
            "test282.jpg, prob: 0.40822312235832214\n",
            "test283.jpg, prob: 0.3279925584793091\n",
            "test284.jpg, prob: 0.9875181317329407\n",
            "test285.jpg, prob: 0.10444042831659317\n",
            "test286.jpg, prob: 0.003421701956540346\n",
            "test287.jpg, prob: 0.0030522558372467756\n",
            "test288.jpg, prob: 0.02635163441300392\n",
            "test289.jpg, prob: 0.015552720054984093\n",
            "test290.jpg, prob: 0.035725634545087814\n",
            "test291.jpg, prob: 0.10211734473705292\n",
            "test292.jpg, prob: 0.42696940898895264\n",
            "test293.jpg, prob: 0.10667014867067337\n",
            "test294.jpg, prob: 0.011632707901299\n",
            "test295.jpg, prob: 0.02864946983754635\n",
            "test296.jpg, prob: 0.3362228274345398\n",
            "test297.jpg, prob: 0.0019350427901372313\n",
            "test298.jpg, prob: 0.02614806592464447\n",
            "test299.jpg, prob: 0.06101198121905327\n",
            "test300.jpg, prob: 0.01029933337122202\n",
            "test301.jpg, prob: 0.3597365617752075\n",
            "test302.jpg, prob: 0.0002805592957884073\n",
            "test303.jpg, prob: 0.6948503255844116\n",
            "test304.jpg, prob: 0.016556810587644577\n",
            "test305.jpg, prob: 0.0032235262915492058\n",
            "test306.jpg, prob: 0.22406820952892303\n",
            "test307.jpg, prob: 0.03037218376994133\n",
            "test308.jpg, prob: 0.10739852488040924\n",
            "test309.jpg, prob: 0.03226567432284355\n",
            "test310.jpg, prob: 0.171855166554451\n",
            "test311.jpg, prob: 0.008753708563745022\n",
            "test312.jpg, prob: 0.008985166437923908\n",
            "test313.jpg, prob: 0.45505955815315247\n",
            "test314.jpg, prob: 5.820605656481348e-05\n",
            "test315.jpg, prob: 0.2572416663169861\n",
            "test316.jpg, prob: 0.5252910852432251\n",
            "test317.jpg, prob: 0.025612011551856995\n",
            "test318.jpg, prob: 0.3987545073032379\n",
            "test319.jpg, prob: 0.42210152745246887\n",
            "test320.jpg, prob: 0.0018727615242823958\n",
            "test321.jpg, prob: 0.9805418252944946\n",
            "test322.jpg, prob: 0.8360866904258728\n",
            "test323.jpg, prob: 0.9973422884941101\n",
            "test324.jpg, prob: 0.9031347036361694\n",
            "test325.jpg, prob: 0.183954656124115\n",
            "test326.jpg, prob: 0.00897836685180664\n",
            "test327.jpg, prob: 0.6358862519264221\n",
            "test328.jpg, prob: 0.02299969643354416\n",
            "test329.jpg, prob: 0.5832926630973816\n",
            "test330.jpg, prob: 0.979947030544281\n",
            "test331.jpg, prob: 0.025162963196635246\n",
            "test332.jpg, prob: 0.8841759562492371\n",
            "test333.jpg, prob: 0.1477803736925125\n",
            "test334.jpg, prob: 0.0002980826247949153\n",
            "test335.jpg, prob: 0.9996520280838013\n",
            "test336.jpg, prob: 0.8856067657470703\n",
            "test337.jpg, prob: 0.007341286633163691\n",
            "test338.jpg, prob: 0.9822889566421509\n",
            "test339.jpg, prob: 0.006256834603846073\n",
            "test340.jpg, prob: 0.9948299527168274\n",
            "test341.jpg, prob: 0.9396430850028992\n",
            "test342.jpg, prob: 0.9733592867851257\n",
            "test343.jpg, prob: 0.7909803986549377\n",
            "test344.jpg, prob: 0.6267854571342468\n",
            "test345.jpg, prob: 0.13795427978038788\n",
            "test346.jpg, prob: 0.9960049986839294\n",
            "test347.jpg, prob: 0.002626969711855054\n",
            "test348.jpg, prob: 0.9975548386573792\n",
            "test349.jpg, prob: 0.012631417252123356\n",
            "test350.jpg, prob: 0.002259747125208378\n",
            "test351.jpg, prob: 0.9999516010284424\n",
            "test352.jpg, prob: 0.9663764834403992\n",
            "test353.jpg, prob: 0.9917784333229065\n",
            "test354.jpg, prob: 0.015741195529699326\n",
            "test355.jpg, prob: 0.01099715381860733\n",
            "test356.jpg, prob: 0.0013344421749934554\n",
            "test357.jpg, prob: 0.06895826011896133\n",
            "test358.jpg, prob: 0.00035113884950987995\n",
            "test359.jpg, prob: 0.010656853206455708\n",
            "test360.jpg, prob: 0.08928199857473373\n",
            "test361.jpg, prob: 0.7989895939826965\n",
            "test362.jpg, prob: 0.06435268372297287\n",
            "test363.jpg, prob: 0.2457570880651474\n",
            "test364.jpg, prob: 0.061308711767196655\n",
            "test365.jpg, prob: 0.023531047627329826\n",
            "test366.jpg, prob: 0.0016543491510674357\n",
            "test367.jpg, prob: 0.009363790974020958\n",
            "test368.jpg, prob: 0.007244496140629053\n",
            "test369.jpg, prob: 0.000761179719120264\n",
            "test370.jpg, prob: 0.4365481436252594\n",
            "test371.jpg, prob: 0.9970314502716064\n",
            "test372.jpg, prob: 0.9911030530929565\n",
            "test373.jpg, prob: 0.0047873640432953835\n",
            "test374.jpg, prob: 0.0016931648133322597\n",
            "test375.jpg, prob: 0.000892980839125812\n",
            "test376.jpg, prob: 0.21028950810432434\n",
            "test377.jpg, prob: 0.998933732509613\n",
            "test378.jpg, prob: 0.00099226797465235\n",
            "test379.jpg, prob: 0.1290443390607834\n",
            "test380.jpg, prob: 0.8907222747802734\n",
            "test381.jpg, prob: 0.22045059502124786\n",
            "test382.jpg, prob: 0.011756904423236847\n",
            "test383.jpg, prob: 0.0020450495649129152\n",
            "test384.jpg, prob: 0.11690256744623184\n",
            "test385.jpg, prob: 0.16273097693920135\n",
            "test386.jpg, prob: 0.0024372334592044353\n",
            "test387.jpg, prob: 0.9843876361846924\n",
            "test388.jpg, prob: 0.008811883628368378\n",
            "test389.jpg, prob: 0.015402923338115215\n",
            "test390.jpg, prob: 0.8254232406616211\n",
            "test391.jpg, prob: 0.02583731710910797\n",
            "test392.jpg, prob: 0.016024621203541756\n",
            "test393.jpg, prob: 0.986024796962738\n",
            "test394.jpg, prob: 0.9841852784156799\n",
            "test395.jpg, prob: 0.036309681832790375\n",
            "test396.jpg, prob: 0.04067628085613251\n",
            "test397.jpg, prob: 0.12825550138950348\n",
            "test398.jpg, prob: 0.062092043459415436\n",
            "test399.jpg, prob: 0.1469256579875946\n",
            "test400.jpg, prob: 0.03506617248058319\n",
            "test401.jpg, prob: 0.03441735729575157\n",
            "test402.jpg, prob: 0.0021749373991042376\n",
            "test403.jpg, prob: 0.040779270231723785\n",
            "test404.jpg, prob: 0.028650300577282906\n",
            "test405.jpg, prob: 0.0013697402318939567\n",
            "test406.jpg, prob: 0.01637105830013752\n",
            "test407.jpg, prob: 0.005069672130048275\n",
            "test408.jpg, prob: 0.9960313439369202\n",
            "test409.jpg, prob: 0.008463439531624317\n",
            "test410.jpg, prob: 0.9293457865715027\n",
            "test411.jpg, prob: 0.01569279097020626\n",
            "test412.jpg, prob: 0.014605248346924782\n",
            "test413.jpg, prob: 0.02693837881088257\n",
            "test414.jpg, prob: 0.01728172041475773\n",
            "test415.jpg, prob: 0.9465360045433044\n",
            "test416.jpg, prob: 0.02930833026766777\n",
            "test417.jpg, prob: 0.9863693714141846\n",
            "test418.jpg, prob: 0.022414958104491234\n",
            "test419.jpg, prob: 0.003973099868744612\n",
            "test420.jpg, prob: 0.0714774876832962\n",
            "test421.jpg, prob: 0.9935898780822754\n",
            "test422.jpg, prob: 0.7844743132591248\n",
            "test423.jpg, prob: 0.5813250541687012\n",
            "test424.jpg, prob: 0.02482212334871292\n",
            "test425.jpg, prob: 0.1747686266899109\n",
            "test426.jpg, prob: 0.0024396474473178387\n",
            "test427.jpg, prob: 0.000728612532839179\n",
            "test428.jpg, prob: 0.2910360097885132\n",
            "test429.jpg, prob: 0.11914104968309402\n",
            "test430.jpg, prob: 0.040342219173908234\n",
            "test431.jpg, prob: 0.0011774859158322215\n",
            "test432.jpg, prob: 0.012206558138132095\n",
            "test433.jpg, prob: 0.0026633976958692074\n",
            "test434.jpg, prob: 0.014998423866927624\n",
            "test435.jpg, prob: 0.11276892572641373\n",
            "test436.jpg, prob: 0.009930762462317944\n",
            "test437.jpg, prob: 0.9245736598968506\n",
            "test438.jpg, prob: 0.1648285835981369\n",
            "test439.jpg, prob: 0.004686855711042881\n",
            "test440.jpg, prob: 0.0023582782596349716\n",
            "test441.jpg, prob: 0.9995279312133789\n",
            "test442.jpg, prob: 0.9609957933425903\n",
            "test443.jpg, prob: 0.3842310309410095\n",
            "test444.jpg, prob: 0.9998316764831543\n",
            "test445.jpg, prob: 0.008845231495797634\n",
            "test446.jpg, prob: 0.9927914142608643\n",
            "test447.jpg, prob: 0.67173832654953\n",
            "test448.jpg, prob: 0.0023824921809136868\n",
            "test449.jpg, prob: 0.0895271822810173\n",
            "test450.jpg, prob: 0.7604246735572815\n",
            "test451.jpg, prob: 0.020656472072005272\n",
            "test452.jpg, prob: 0.880713164806366\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = load_resnet_best_percent()\n",
        "model.eval()\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "predictions = []\n",
        "image_names = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, filenames in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        image_names.extend(filenames)\n",
        "\n",
        "        probabilities = nn.Softmax(dim=1)(outputs.clone().detach())\n",
        "\n",
        "        probs_class_1 = probabilities[:, 1].cpu().numpy()\n",
        "        all_probs.extend(probs_class_1)\n",
        "\n",
        "print('predictions made')\n",
        "\n",
        "\n",
        "all_probs = np.array(all_probs)\n",
        "for i in range(0, int(len(all_probs)*0.1)):\n",
        "    print(f'test{i+1}.jpg, prob: {all_probs[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cac1138-ac37-454d-8634-4ca9dad916ad",
      "metadata": {
        "id": "9cac1138-ac37-454d-8634-4ca9dad916ad",
        "outputId": "7b892c1a-7450-4f35-a71c-cbbab29a9f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Значения записаны в столбец target_people и сохранены в updated_sample_submission.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "submission_df = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "if len(submission_df) == len(all_probs):\n",
        "    submission_df['target_people'] = all_probs\n",
        "\n",
        "    submission_df.to_csv('updated_sample_submission.csv', index=False)\n",
        "    print(\"Значения записаны в столбец target_people и сохранены в updated_sample_submission.csv.\")\n",
        "else:\n",
        "    print(\"Размерности не совпадают. Проверьте данные.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}